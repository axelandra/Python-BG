{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimización de una función con TensorFlow 2.x\n",
    "\n",
    "Este cuaderno pretende ilustrar las diferencias entre la versión de TensorFlow 2.x y la versión 1.x utilizando el cuaderno introductorio `minimizacion-tf.ipynb`. En el siguiente enlace podrá encontrar una descripción más detallada de las diferencias entre ambas versiones del framwork. \n",
    "- [https://www.tensorflow.org/guide/effective_tf2](https://www.tensorflow.org/guide/effective_tf2)\n",
    "\n",
    "Aunque la versión de TensorFlow 2.0 fue liberada oficialmente en septiembre de 2019, aún se encuentran todavía en la red muchos ejemplos y cursos en línea que utilizan la versión 1.x, por lo que es todavía más didáctico aprender ML con la versión 1. Sin embargo, las mejoras introducidas en la versión 2 permiten un flujo de trabajo más intuitivo y fluido en la mayoría de los casos; y eventualmente, se dejará de dar soporte a la versión 1 de TensorFlow, por lo que es conveniente introducir la utilización de la versión 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.1.0'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función *banana* de Rosenbrock\n",
    "\n",
    "La función banana se utiliza como *benchmark* en algoritmos de optimización numérica, por la forma tan peculiar de sus curvas de nivel. En este ejemplo, vamos a ilustrar cómo podemos utilizar el algoritmo de optimización denominado Adam para optimizar la función: \n",
    "$$ f(x,y) = (1-x)^2 + 100(y-x^2)^2 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banana function](https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Rosenbrock_function.svg/600px-Rosenbrock_function.svg.png)\n",
    "\n",
    "Ahora vamos a definir la en TensorFlow, quien se encargará de construir el gráfico de computación y nos permitirá obtener sus gradientes a través de **diferenciación automática**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Eager execution*\n",
    "\n",
    "Vemos que ahora al crear las variables de nuestra función a optimizar, están evaluadas automáticamente. A este modo de ejecución del gráfico de computación se le conoce como *eager execution* y es una de las características nuevas introducidas en TensorFlow 2.x. Anteriormente, en la versión 1.x era necesario primero crear un gráfico de computación y luego ejecutarlo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.6871292], dtype=float32)>,\n <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-0.44555563], dtype=float32)>)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# tf.reset_default_graph() # esta función ya no existe\n",
    "# Creamos las variables de la función\n",
    "x = tf.Variable(tf.random.normal(shape = (1,), seed = 212))\n",
    "y = tf.Variable(tf.random.normal(shape = (1,), seed = 213))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora para poder definir nuestra función a optimizar, lo haremos creando una función de Python, en vez de un tensor, como en la versión 1. El decorador `@tf.function` le indica a Python que debe construir un gráfico de computación de TensorFlow y permite que la evaluación de la función sea más eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.eager.def_function.Function at 0x20ed24b5d08>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Definición de la función: notar ahora el decorador @tf.function \n",
    "@tf.function\n",
    "def f(x,y):\n",
    "    f = (1-x)**2 + 100*(y-x**2)**2\n",
    "    return f\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de las sesiones\n",
    "\n",
    "En la versión 2 de TensorFlow ¡ya no existen las sesiones!, por lo que el siguiente código arroja un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-36f3c5a68911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluamos la función en su mínimo global\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mminimo_global\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'El mínimo global es %0.2f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mminimo_global\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "# Evaluamos la función en su mínimo global\n",
    "with tf.Session() as sess:\n",
    "    minimo_global = sess.run(f, feed_dict={x:(1,), y:(1,)})\n",
    "\n",
    "print('El mínimo global es %0.2f' % minimo_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora las sesiones se simplifican utilizando el gráfico de computación definido en las funciones. Ya que TensorFlow 2 opera en modo de *eager execution*, para obtener el valor de $f(x,y)$ basta con hacer una intuitiva llamada de función: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([84.31562], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "f(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=int32, numpy=0>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "f(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización con gradiente en descenso (*gradient descent*)\n",
    "\n",
    "Anteriormente, construimos un *loop* de optimización utilizando una sesión de TensorFlow. Ya que ahora las sesiones fueron reemplazadas por funciones, lo conveniente es crear una función que realice un paso del algoritmo de gradiente en descenso. Aunque esto será lo expuesto en el código a continuación, se recomienda que en los problemas de optimización de ML se utilicen las funciones y modelos de la interfaz de alto nivel Keras incluida en TensorFlow 2, por lo que a continuación se mostrará un \"equivalente\" al código de la versión 1.x. \n",
    "\n",
    "Vamos a crear el \"nodo optimizador\" que implemente el algoritmo Adam. Ahora este algoritmo es parte del subpaquete de Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x20ed8a94908>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, creamos una función que devuelve el valor de la función objetivo y los gradientes de la función, esta función nos servirá para darle estos gradientes al nodo optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.eager.def_function.Function at 0x20f756b9c48>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "@tf.function\n",
    "def grad(f, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        f_val = f(x, y)\n",
    "    return f_val, tape.gradient(f_val, [x, y])\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá podemos ver cómo funcionaría un único paso de optimización. Para aplicar un paso del gradiente en descenso, utilizamos el método `apply_gradients` del nodo optimizador, el cual recibe un iterable de los gradientes y las variables respecto a las que fueron computados (las variables de control). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Paso: 0, Valor de f: [84.31562]\nPaso: 1, Valor de f: [64.36963]\n"
    }
   ],
   "source": [
    "# Valores iniciales de f y sus gradientes\n",
    "f_val, grads = grad(f, x, y)\n",
    "print(\"Paso: {}, Valor de f: {}\".format(optimizer.iterations.numpy(), f_val.numpy()))\n",
    "# print(grads)\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, [x, y]))\n",
    "print(\"Paso: {}, Valor de f: {}\".format(optimizer.iterations.numpy(), f(x, y).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\t(0.59, -0.35):\t64.3696\n100\t(0.15, 0.02):\t0.7249\n200\t(0.35, 0.12):\t0.4299\n300\t(0.50, 0.24):\t0.2558\n400\t(0.61, 0.37):\t0.1562\n500\t(0.69, 0.47):\t0.0969\n600\t(0.76, 0.57):\t0.0604\n700\t(0.81, 0.65):\t0.0376\n800\t(0.85, 0.72):\t0.0232\n900\t(0.88, 0.78):\t0.0141\n1000\t(0.91, 0.83):\t0.0084\n1100\t(0.93, 0.86):\t0.0049\n1200\t(0.95, 0.90):\t0.0028\n1300\t(0.96, 0.92):\t0.0016\n1400\t(0.97, 0.94):\t0.0008\n1500\t(0.98, 0.96):\t0.0004\n1600\t(0.99, 0.97):\t0.0002\n1700\t(0.99, 0.98):\t0.0001\n1800\t(0.99, 0.99):\t0.0000\n1900\t(1.00, 0.99):\t0.0000\n2000\t(1.00, 0.99):\t0.0000\n2100\t(1.00, 1.00):\t0.0000\n2200\t(1.00, 1.00):\t0.0000\n2300\t(1.00, 1.00):\t0.0000\n2400\t(1.00, 1.00):\t0.0000\n"
    }
   ],
   "source": [
    "# Ahora creamos un ciclo de optimización, que aplica pasos de optimización \n",
    "# sucesivamente durante N iteraciones\n",
    "N = 2500\n",
    "for i in range(N):\n",
    "    # Valores iniciales de f y sus gradientes\n",
    "    f_val, grads = grad(f, x, y)\n",
    "\n",
    "    # Aplicamos un paso de gradiente en descenso\n",
    "    optimizer.apply_gradients(zip(grads, [x, y]))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"%i\\t(%0.2f, %0.2f):\\t%0.4f\" % (i, x.numpy(), y.numpy(), f_val.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora obtenemos los resultados del proceso de optimización directamente en los tensores `x` e `y`, que fueron actualizadas interactivamente durante cada uno de los pasos de optimización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.9997714], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.99954224], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión \n",
    "\n",
    "Como vemos, con la versión 2.x de Tensorflow, la mayoría del proceso de definición de la función quedó más intuitivamente expresado en el código de Python, respecto a la versión 1.x. En general, la optimización y definición de los modelos se hará con la interfaz de alto nivel Keras, por lo que el código acá es solamente con fines ilustrativos, aunque puede ser extendido para proveer mayor flexibilidad o incorporación de flujos de trabajo muy específicos y no definidos en Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596221897962"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
